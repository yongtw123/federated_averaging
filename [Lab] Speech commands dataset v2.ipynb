{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if in correct environment\n",
    "import sys, os, time, datetime\n",
    "is_conda = os.path.exists(os.path.join(sys.prefix, 'conda-meta'))\n",
    "if not is_conda:\n",
    "    sys.exit('Not in conda, check environment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import shelve\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default args (convenience class)\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.network = 'lenet5'\n",
    "        self.optim = 'sgd'\n",
    "        self.lr_scheduler = 'lambda'\n",
    "        self.description = ''\n",
    "        \n",
    "        self.seed = 1\n",
    "        self.log_frequency = 2\n",
    "        self.use_cuda = True\n",
    "        self.save_model = False\n",
    "        \n",
    "        self.batch_size = 10\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = 'auto'        \n",
    "        # For fed learn this means each worker has own shuffler, does 'NOT' mean data federated randomly to workers\n",
    "        self.shuffle = True\n",
    "        # Mutually exclusive with shuffle\n",
    "        self.use_weighted_random_sampler = False        \n",
    "        self.weight_decay = [0, 1e-2, 1e-4, 1e-7][3]\n",
    "        self.lr = [1e0, 1e-1, 5e-2, 1e-2, 5e-3, 1e-3, 1e-4, 1e-5][3]\n",
    "        \n",
    "        ### lr scheduler plateau ###\n",
    "        # Number of epochs with no improvement after which learning rate will be reduced\n",
    "        # =\"will tolerate 2 bad epochs, if next epoch is bad, step, if not, reset bad epoch num to zero\"\n",
    "        self.lr_scheduler_patience = 2\n",
    "        # lr*gamma to decrease\n",
    "        self.lr_scheduler_gamma = 0.1 \n",
    "        ### lr scheduler lambda ###\n",
    "        self.lr_lambda = lambda epoch: 1.0 / (1+epoch) # decrease fast in the begining\n",
    "        #self.lr_lambda = lambda epoch: 0.9 ** epoch # decrease slow, but may become too large in later epochs\n",
    "        #self.lr_lambda = lambda epoch: 1.0 # dummy lr\n",
    "        \n",
    "        self.federate = True\n",
    "        self.rounds = 2\n",
    "        self.epoch_per_round = 1\n",
    "        self.global_model = True\n",
    "        self.fedadam = False\n",
    "        self.fedprox = False\n",
    "        # NOTE: v1 does not include buffers eg. means/variances (b/c using named_parameters())\n",
    "        #       v2 use state_dict()\n",
    "        #       v3 mt-fedavg\n",
    "        self.fedavgver = 2\n",
    "        self.fed_worker_num = 10\n",
    "        self.class_per_worker = 5\n",
    "        self.dist_scheme = 'permuted'\n",
    "        self.custom_mapping = {0: [0, 1, 3, 4, 5], 1: [2, 6, 7, 8, 9], 2: [1, 4, 5, 6, 8], 3: [0, 2, 3, 7, 9],\n",
    "                               4: [1, 2, 4, 5, 9], 5: [0, 3, 6, 7, 8], 6: [1, 2, 3, 5, 6], 7: [0, 4, 7, 8, 9],\n",
    "                               8: [2, 3, 4, 8, 9], 9: [0, 1, 5, 6, 7]}\n",
    "#         self.custom_mapping = {0: [9, 1], 1: [4, 7], 2: [2, 5], 3: [3, 6], 4: [0, 8],\n",
    "#                                5: [8, 6], 6: [0, 2], 7: [3, 7], 8: [1, 5], 9: [9, 4],\n",
    "#                                10:[0, 2],11: [1, 6],12: [3, 5],13: [9, 7],14: [8, 4],\n",
    "#                                15:[9, 6],16: [4, 5],17: [0, 1],18: [8, 3],19: [2, 7]}\n",
    "        self.uniqueness_threshold = 3 # for scheme 'choose-unique': max number of same class for all workers\n",
    "        \n",
    "        self.use_pysyft = True\n",
    "        self.pysyft_worker_verbose = True\n",
    "        self.pysyft_worker_def = {\n",
    "            'charlie': { 'type': 'websocket', 'port': 8779 },\n",
    "            'alice': { 'type': 'websocket', 'port': 8777 },\n",
    "            'bob': { 'type': 'websocket', 'port': 8778 },\n",
    "            'dave': { 'type': 'virtual' },\n",
    "            'eve': { 'type': 'virtual' },\n",
    "            'fred': { 'type': 'virtual' },\n",
    "            'george': { 'type': 'virtual' },\n",
    "            'harry': { 'type': 'virtual' },\n",
    "            'ian': { 'type': 'virtual' },\n",
    "            'jack': { 'type': 'virtual' },\n",
    "        } # BUG: Pysyft expects worker id as string; if to use dist_scheme=custom must change key\n",
    "        # TODO: Cannot mix GPU and CPU, only one kind of device throughout\n",
    "        \n",
    "        self.dataset_root = 'speech_commands_dataset_v2'\n",
    "        self.use_cache = True\n",
    "        self.cache = 'speech_commands_dataset_v2.cache'        \n",
    "        self.datalist_root = 'datalists'\n",
    "        self.datalist = ['numbers_noise', 'leftright', 'numbers'][2]\n",
    "        self.train_dataset = 'training_list.txt'\n",
    "        self.valid_dataset = 'validation_list.txt'\n",
    "        self.test_dataset = 'testing_list.txt'\n",
    "        self.classlabels = '__classes__.txt'\n",
    "        self.dataload_workers_num = 0\n",
    "        self.drop_last_batch = True\n",
    "        \n",
    "        # Checks\n",
    "        if self.use_cuda:\n",
    "            if torch.cuda.is_available():\n",
    "                self.default_device = 'cuda'\n",
    "                self.cuda_args = {'num_workers': 1, 'pin_memory': True}\n",
    "            else:\n",
    "                print('CUDA not available, using CPU instead.')\n",
    "                self.use_cuda = False\n",
    "                self.default_device = 'cpu'\n",
    "        else:\n",
    "            self.default_device = 'cpu'\n",
    "        \n",
    "        if self.use_pysyft:\n",
    "            if (self.fed_worker_num > len(self.pysyft_worker_def)):\n",
    "                print('Not enought workers defined in pysyft_worker_def!')\n",
    "                sys.exit('Aborted.')\n",
    "            for idx in self.pysyft_worker_def:\n",
    "                if 'device' not in self.pysyft_worker_def[idx]:\n",
    "                    self.pysyft_worker_def[idx]['device'] = self.default_device\n",
    "        \n",
    "        if self.fedadam and self.optim != 'adam':\n",
    "            print('FedAdam specified but optimizer is not Adam.')\n",
    "            self.fedadam = False\n",
    "            \n",
    "        if self.fedavgver == 3 and not self.global_model:\n",
    "            print('federated_average version 3 requires using a global model. Setting global_model to True.')\n",
    "            self.global_model = True\n",
    "            \n",
    "        if self.epochs == 'auto':\n",
    "            self.epochs = self.rounds * self.epoch_per_round if self.federate else self.rounds\n",
    "            \n",
    "        if self.fedprox:\n",
    "            self.weight_decay = 0\n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "START_TIMESTAMP = int(time.time()*1000)\n",
    "START_EPOCH = 0\n",
    "\n",
    "BEST_ACC = 0\n",
    "LEAST_LOSS = 1e100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "from transformation import *\n",
    "\n",
    "audio_feature_transform = Compose([\n",
    "    FixAudioLength(),\n",
    "    ToMelSpectrogram(),\n",
    "    ToTensorFromSpect()\n",
    "])\n",
    "tensor_transform = Compose([\n",
    "    ToTensorFromSpect()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataloader (iterator), instantiate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9}\n",
      "{'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9}\n",
      "{'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9}\n"
     ]
    }
   ],
   "source": [
    "from dataset import SpeechCommandsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "transform = tensor_transform if args.use_cache else audio_feature_transform\n",
    "db = shelve.open(args.cache, 'r') if args.use_cache else None\n",
    "\n",
    "paths = [{\n",
    "    'classes': os.path.join(args.datalist_root, args.datalist, args.classlabels),\n",
    "    'filelist': os.path.join(args.datalist_root, args.datalist, filelist),\n",
    "    'dataset_dir': args.dataset_root\n",
    "} for filelist in [args.train_dataset, args.valid_dataset, args.test_dataset]]\n",
    "train_dataset = SpeechCommandsDataset(paths[0], transform=transform, db=db)\n",
    "valid_dataset = SpeechCommandsDataset(paths[1], transform=transform, db=db)\n",
    "test_dataset = SpeechCommandsDataset(paths[2], transform=transform, db=db)\n",
    "\n",
    "# Get number of classes (same for train/valid/test)\n",
    "nclass = train_dataset.get_num_of_classes()\n",
    "\n",
    "sampler = None\n",
    "if args.use_weighted_random_sampler:\n",
    "    # Adopted from https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\n",
    "    weights_for_sampling = train_dataset.make_weights_for_balanced_classes()\n",
    "    sampler = WeightedRandomSampler(weights_for_sampling, len(weights_for_sampling))\n",
    "\n",
    "# Note: train dataloader may be overriden if args.federate is True\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=args.shuffle,\n",
    "                              pin_memory=args.use_cuda, num_workers=args.dataload_workers_num,\n",
    "                              sampler=sampler)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=args.test_batch_size, shuffle=args.shuffle,\n",
    "                              pin_memory=args.use_cuda, num_workers=args.dataload_workers_num)\n",
    "test_dataloader  = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=args.shuffle,\n",
    "                              pin_memory=args.use_cuda, num_workers=args.dataload_workers_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import resnet as resnet\n",
    "import densenet as densenet\n",
    "import mobilenet as mn\n",
    "import googlenet as googlenet\n",
    "import lenet as lenet\n",
    "import squeezenet as squeezenet\n",
    "import shufflenetv2 as shufflenetv2\n",
    "import nasnet as nasnet\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from fedavg import *\n",
    "\n",
    "hyperparam = {\"num_classes\": nclass, \"in_channels\": 1}\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(network='mobilenetv2', send_to_device=None, rootmodel=None):\n",
    "    if network == 'mobilenetv2':\n",
    "        model = mn.mobilenet_v2(**hyperparam)\n",
    "    elif network == 'mobilenetv2_quantize':\n",
    "        model = mn.mobilenet_v2q(**hyperparam)\n",
    "    elif network == 'mobilenetv1':\n",
    "        model = mn.mobilenet_v1(**hyperparam)\n",
    "    elif network == 'mobilenetv1_quantize':\n",
    "        model = mn.mobilenet_v1q(**hyperparam)\n",
    "    elif network == 'googlenet':\n",
    "        model = googlenet.googlenet(**hyperparam)\n",
    "    elif network == 'squeezenet1_1':\n",
    "        model = squeezenet.squeezenet1_1(**hyperparam)\n",
    "    elif network == 'efficientnetb0':\n",
    "        model = EfficientNet.from_name('efficientnet-b0', image_size=32, **hyperparam)\n",
    "    elif network == 'shufflenetv2_x0.5':\n",
    "        model = shufflenetv2.shufflenet_v2_x0_5(**hyperparam)\n",
    "    elif network == 'shufflenetv2_x1':\n",
    "        model = shufflenetv2.shufflenet_v2_x1_0(**hyperparam)\n",
    "    elif network == 'resnet18':\n",
    "        model = resnet.resnet18(**hyperparam)\n",
    "    elif network == 'resnet50':\n",
    "        model = resnet.resnet50(**hyperparam)\n",
    "    elif network == 'densenet121':\n",
    "        model = densenet.densenet121(**hyperparam)\n",
    "    elif network == 'nasnet-a-mobile':\n",
    "        model = nasnet.nasnet_a_mobile(**hyperparam)\n",
    "    elif network == 'lenet5':\n",
    "        model = lenet.LeNet5() # Already accepting 1x32x32, output 10\n",
    "    else:\n",
    "        raise ValueError('Bad network name \"%s\" supplied for create_model.' % network)\n",
    "\n",
    "    # For multi-GPU\n",
    "    #if args.use_cuda:\n",
    "        #model = torch.nn.DataParallel(model).to(DEVICE)\n",
    "        \n",
    "    if rootmodel:\n",
    "        #copy_model(model, rootmodel)\n",
    "        model.load_state_dict(rootmodel.state_dict())\n",
    "    \n",
    "    if send_to_device:\n",
    "        model = model.to(torch.device(send_to_device))\n",
    "        \n",
    "    return model\n",
    "        \n",
    "def create_optimizer(model, algorithm='sgd'):\n",
    "    # Define optimizer\n",
    "    if algorithm == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    elif algorithm == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    elif algorithm == 'rmsprop':    \n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        raise ValueError('Bad algorithm named \"%s\" supplied for create_optimizer.' % algorithm)\n",
    "    return optimizer\n",
    "\n",
    "def create_lr_scheduler(optimizer, algorithm='lambda'):\n",
    "    # Define learning rate modifier\n",
    "    if algorithm == 'lambda':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=args.lr_lambda, last_epoch=-1)\n",
    "    elif algorithm == 'plateau':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=args.lr_scheduler_patience, \n",
    "                                                                  factor=args.lr_scheduler_gamma)\n",
    "    else:\n",
    "        raise ValueError('Bad algorithm named \"%s\" supplied for create_lr_scheduler.' % algorithm)\n",
    "    return lr_scheduler\n",
    "\n",
    "def get_lr(opt):\n",
    "    \"\"\"Reads current learning rate from a torch.optim\"\"\"\n",
    "    \n",
    "    return opt.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending data to worker dave ...\n",
      "Sending data to worker alice ...\n",
      "Sending data to worker charlie ...\n",
      "Sending data to worker bob ...\n",
      "Sending data to worker eve ...\n",
      "Sending data to worker fred ...\n",
      "Sending data to worker george ...\n",
      "Sending data to worker harry ...\n",
      "Sending data to worker ian ...\n",
      "Sending data to worker jack ...\n",
      "Worker <VirtualWorker id:dave #objects:2>, number of input: 3136, classes: [0, 1, 2, 4, 5]\n",
      "Worker <WebsocketClientWorker id:alice #tensors local:0 #tensors remote: 2>, number of input: 3086, classes: [3, 6, 7, 8, 9]\n",
      "Worker <WebsocketClientWorker id:charlie #tensors local:0 #tensors remote: 2>, number of input: 3121, classes: [0, 1, 4, 6, 9]\n",
      "Worker <WebsocketClientWorker id:bob #tensors local:0 #tensors remote: 2>, number of input: 3113, classes: [2, 3, 5, 7, 8]\n",
      "Worker <VirtualWorker id:eve #objects:2>, number of input: 3120, classes: [1, 2, 3, 7, 9]\n",
      "Worker <VirtualWorker id:fred #objects:2>, number of input: 3114, classes: [0, 4, 5, 6, 8]\n",
      "Worker <VirtualWorker id:george #objects:2>, number of input: 3119, classes: [0, 2, 4, 5, 8]\n",
      "Worker <VirtualWorker id:harry #objects:2>, number of input: 3115, classes: [1, 3, 6, 7, 9]\n",
      "Worker <VirtualWorker id:ian #objects:2>, number of input: 3104, classes: [0, 1, 3, 4, 7]\n",
      "Worker <VirtualWorker id:jack #objects:2>, number of input: 3130, classes: [2, 5, 6, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "if not args.federate:\n",
    "    # Centralized mode\n",
    "    model = create_model(send_to_device=args.default_device, network=args.network)\n",
    "    optimizer = create_optimizer(model, algorithm=args.optim)\n",
    "    lr_scheduler = create_lr_scheduler(optimizer, algorithm=args.lr_scheduler)\n",
    "else:\n",
    "    # Federated learning mode    \n",
    "    # (1) With Pysyft\n",
    "    if args.use_pysyft:\n",
    "        import syft as sy\n",
    "        hook = sy.TorchHook(torch)  # decorate torch lib\n",
    "        \n",
    "        # Prepare workers\n",
    "        workers = []\n",
    "        for idx, (worker_id, worker_def) in enumerate(args.pysyft_worker_def.items()):\n",
    "            if idx > args.fed_worker_num-1: \n",
    "                break\n",
    "            if worker_def['type'] == 'websocket':\n",
    "                remote_worker = sy.WebsocketClientWorker(hook, host='localhost', port=worker_def['port'], id=worker_id, timeout=600)\n",
    "                if (remote_worker.objects_count_remote() > 0):\n",
    "                    remote_worker.clear_objects_remote()\n",
    "                workers.append(remote_worker)\n",
    "            else:\n",
    "                workers.append(sy.VirtualWorker(hook, id=worker_id))\n",
    "        \n",
    "        # Distribute dataset\n",
    "        worker_ids = [w.id for w in workers]    \n",
    "        fed_dataset, meta_data = federate_dataset(train_dataset, workers, nclass, args.dist_scheme,\n",
    "                                                  uniqueness_threshold=args.uniqueness_threshold,\n",
    "                                                  class_per_worker=args.class_per_worker,\n",
    "                                                  custom_mapping=args.custom_mapping,\n",
    "                                                  use_pysyft=args.use_pysyft)\n",
    "            \n",
    "        # Overwrite train dataloader to be federated\n",
    "        # FederatedDataLoader does not support many options\n",
    "        train_dataloader = sy.FederatedDataLoader(fed_dataset, batch_size=args.batch_size, shuffle=args.shuffle,\n",
    "                                                  drop_last=args.drop_last_batch)\n",
    "    \n",
    "    # (2) Simulate local epochs (faster than Pysyft...)\n",
    "    else:\n",
    "        workers = list(range(args.fed_worker_num)) # workers is a list of ids\n",
    "        fed_dataset, meta_data = federate_dataset(train_dataset, workers, nclass, args.dist_scheme,\n",
    "                                                  uniqueness_threshold=args.uniqueness_threshold,\n",
    "                                                  class_per_worker=args.class_per_worker,\n",
    "                                                  custom_mapping=args.custom_mapping,\n",
    "                                                  use_pysyft=args.use_pysyft)\n",
    "            \n",
    "        # train_dataloader will now become a Dict of DataLoaders\n",
    "        from torch.utils.data import TensorDataset\n",
    "        worker_ids = workers\n",
    "        train_dataloader = { w: DataLoader(\n",
    "                                  TensorDataset(fed_dataset[w][0], fed_dataset[w][1]), \n",
    "                                  batch_size=args.batch_size, shuffle=args.shuffle,\n",
    "                                  pin_memory=args.use_cuda, num_workers=args.dataload_workers_num,\n",
    "                                  sampler=sampler, drop_last=args.drop_last_batch)\n",
    "                            for w in workers\n",
    "                          }\n",
    "    \n",
    "    # Tally local dataset size\n",
    "    worker_n = {}\n",
    "    for w in workers:\n",
    "        dim0 = meta_data[w]['xshape'][0]\n",
    "        worker_n[w.id if args.use_pysyft else w] = dim0\n",
    "        print('Worker %s, number of input: %d, classes: %s' % \\\n",
    "              (str(w), dim0, str(meta_data[w]['yset'])) )\n",
    "    \n",
    "    # models/optimizers/lr_schedulers to be federated\n",
    "    fed_rootmodel = create_model(network=args.network) # same random initialization\n",
    "    fed_models = { id: create_model(\n",
    "        send_to_device=args.default_device, network=args.network, rootmodel=fed_rootmodel) for id in worker_ids }\n",
    "    global_model = create_model(\n",
    "        send_to_device=args.default_device, network=args.network, rootmodel=fed_rootmodel) if args.global_model else None\n",
    "    fed_optimizers = { id: create_optimizer(fed_models[id], algorithm=args.optim) for id in worker_ids }\n",
    "    fed_lr_schedulers = { id: create_lr_scheduler(fed_optimizers[id], algorithm=args.lr_scheduler) for id in worker_ids }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):    \n",
    "    print(\"epoch %3d with lr=%.02e, wall clock %s\" % (epoch, get_lr(optimizer), str(datetime.datetime.now().time())))\n",
    "    log_interval = len(train_dataloader) // args.log_frequency\n",
    "    device = torch.device(args.default_device)\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        #print(targets)\n",
    "        \n",
    "        # copy tensors to cpu/gpu\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward/backward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        #loss = criterion(outputs.logits, targets) # for googlenet\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        if batch_idx == 0 or batch_idx % log_interval == log_interval-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_dataset),\n",
    "                100. * batch_idx / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_federated_pysyft(epoch, models, optimizers, global_model=None):\n",
    "    \"\"\"Return True if is FedAvg round, else return False.\"\"\"\n",
    "    \n",
    "    log_interval = len(train_dataloader) // (args.log_frequency * args.fed_worker_num)\n",
    "    is_fedavg_round = (epoch % args.epoch_per_round == args.epoch_per_round-1)\n",
    "    is_start_round = (epoch % args.epoch_per_round == 0)\n",
    "    device = torch.device(args.default_device)\n",
    "    last_worker = ''\n",
    "        \n",
    "    for worker_id in models:\n",
    "        models[worker_id].train()  # Set model to training mode\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        worker = inputs.location # worker here is Pysyft Worker\n",
    "\n",
    "        model = models[worker.id]\n",
    "        optimizer = optimizers[worker.id]\n",
    "        \n",
    "        if last_worker != worker.id:\n",
    "            print(\"epoch %3d on worker %s with lr=%.02e, wall clock %s\" % \\\n",
    "                  (epoch, worker.id, get_lr(optimizer), str(datetime.datetime.now().time())))\n",
    "            last_worker = worker.id\n",
    "            \n",
    "            #if args.pysyft_worker_def[last_worker]['device'] == 'cuda':\n",
    "            #    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "            #    last_device = torch.device('cuda')\n",
    "            #elif args.pysyft_worker_def[last_worker]['device'] == 'cpu':\n",
    "            #    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "            #    last_device = torch.device('cpu')\n",
    "            \n",
    "            if is_start_round:\n",
    "                #model.to(last_device)\n",
    "                # send model to worker\n",
    "                model.send(worker)\n",
    "        \n",
    "        # copy tensors to cpu/gpu\n",
    "        #inputs = inputs.to(last_device)\n",
    "        #targets = targets.to(last_device)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward/backward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        #loss = criterion(outputs.logits, targets) # for googlenet\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.get()\n",
    "\n",
    "        # statistics\n",
    "        if batch_idx % log_interval == log_interval-1:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_dataset),\n",
    "                100. * batch_idx / len(train_dataloader), batch_loss.item()))\n",
    "        #print(\"Batch %d done\" % batch_idx) # Debug\n",
    "    \n",
    "    if is_fedavg_round:\n",
    "        for worker_id, model in models.items():\n",
    "            model.get() # Pysyft get model back\n",
    "            #model.to(default_device)\n",
    "        fedavg(args.fedavgver, models, worker_n, global_model)\n",
    "        torch.set_default_tensor_type('torch.FloatTensor')\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_federated_sim(epoch, models, optimizers, global_model=None):\n",
    "    \"\"\"Return True if is FedAvg round, else return False.\"\"\"\n",
    "\n",
    "    is_fedavg_round = (epoch % args.epoch_per_round == args.epoch_per_round-1)\n",
    "    #is_start_round = (epoch % args.epoch_per_round == 0)\n",
    "    device = torch.device(args.default_device)\n",
    "    \n",
    "    # NOTE: iterating over worker_id\n",
    "    for worker_id in worker_ids:\n",
    "        model = models[worker_id]\n",
    "        dataloader = train_dataloader[worker_id]\n",
    "        optimizer = optimizers[worker_id]\n",
    "        log_interval = len(dataloader) // args.log_frequency\n",
    "        print(\"Local epoch %2d on worker %2d with lr=%.02e, wall clock %s\" % \\\n",
    "              (epoch, worker_id, get_lr(optimizer), str(datetime.datetime.now().time())))\n",
    "        \n",
    "        \"\"\"# DEBUG\n",
    "        worst_loss = -1\n",
    "        prev_batch = None\n",
    "        # END DEBUG\"\"\"\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            if inputs.size()[0] == 1:\n",
    "                print('[WARNING] Batch size of 1, dropping...')\n",
    "                continue\n",
    "            inputs = torch.unsqueeze(inputs, 1)\n",
    "        \n",
    "            # copy tensors to cpu/gpu\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward/backward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            #loss = criterion(outputs.logits, targets) # for googlenet\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \"\"\"# DEBUG\n",
    "            previous_batch = [inputs, outputs, targets, loss]\n",
    "            if worst_loss == -1:\n",
    "                worst_loss = loss.item()\n",
    "            if torch.isnan(loss).any():\n",
    "                torch.set_printoptions(profile=\"full\")\n",
    "                with open('bad.dump', 'a') as f:\n",
    "                    print('Anomaly at batch %d, loss %f -> %f' % (batch_idx, worst_loss, loss.item()))\n",
    "                    print('epoch %s, worker %s, batch %d, anomaly detected:\\n' % (epoch, worker_id, batch_idx), file=f)\n",
    "                    print(inputs, file=f)\n",
    "                    print(targets, file=f)\n",
    "                    print(outputs, file=f)\n",
    "                    print('Previously:', file=f)\n",
    "                    print(previous_batch, file=f)\n",
    "                    print('=========================================================================\\n', file=f)\n",
    "                torch.set_printoptions(profile=\"default\") # reset\n",
    "                worst_loss = loss.item()\n",
    "            # END DEBUG\"\"\"\n",
    "            \n",
    "            # statistics\n",
    "            if batch_idx % log_interval == log_interval-1:\n",
    "                print('{:3d}/{:3d} batches bs={}\\t({:.0f}%)\\tLoss: {:.6f}'.format(\n",
    "                    batch_idx, len(dataloader), args.batch_size,\n",
    "                    100. * batch_idx / len(dataloader), loss.item()))\n",
    "    \n",
    "    if is_fedavg_round:\n",
    "        fedavg(args.fedavgver, models, worker_n, global_model)\n",
    "        if args.fedadam:\n",
    "            fedadam(optimizers, worker_n)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and test routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(epoch, model):\n",
    "    global BEST_ACC, LEAST_LOSS\n",
    "\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    device = torch.device(args.default_device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for idx, (inputs, targets) in enumerate(valid_dataloader):\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        # statistics\n",
    "        it += 1\n",
    "        running_loss += loss.item()\n",
    "        pred = outputs.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "        correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "        #pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        #correct += pred.eq(targets.data.view_as(pred)).sum()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    epoch_loss = running_loss / it\n",
    "    print('[ROUND] Accuracy: %.4f, Epoch loss: %.4f' % (accuracy, epoch_loss))\n",
    "    \n",
    "    if args.save_model:\n",
    "        #checkpoint = {\n",
    "        #    'epoch': epoch,\n",
    "        #    'state_dict': model.state_dict(),\n",
    "        #    'loss': epoch_loss,\n",
    "        #    'accuracy': accuracy,\n",
    "        #    'optimizer' : optimizer.state_dict(),\n",
    "        #}\n",
    "\n",
    "        # a name used to save checkpoints etc.\n",
    "        full_name = '%s_%s_%s_bs%d_lr%.1e_wd%.1e' % \\\n",
    "                    (args.network, args.optim, args.lr_scheduler, args.batch_size, args.lr, args.weight_decay)\n",
    "\n",
    "        if accuracy > BEST_ACC:\n",
    "            #torch.save(checkpoint, 'checkpoints/best-loss-speech-commands-checkpoint-%s.pth' % full_name)\n",
    "            torch.save(model, '%d-%s-best-acc-%s.pth' % (START_TIMESTAMP, full_name, args.description))\n",
    "        if epoch_loss < LEAST_LOSS:\n",
    "            #torch.save(checkpoint, 'checkpoints/best-acc-speech-commands-checkpoint-%s.pth' % full_name)\n",
    "            torch.save(model, '%d-%s-least-loss-%s.pth' % (START_TIMESTAMP, full_name, args.description))\n",
    "\n",
    "        #torch.save(checkpoint, 'checkpoints/last-speech-commands-checkpoint.pth')\n",
    "        #del checkpoint  # reduce memory\n",
    "    \n",
    "    BEST_ACC = max(accuracy, BEST_ACC)\n",
    "    LEAST_LOSS = min(epoch_loss, LEAST_LOSS)\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "    device = torch.device(args.default_device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    it = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for idx, (inputs, targets) in enumerate(test_dataloader):\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        # statistics\n",
    "        it += 1\n",
    "        running_loss += loss.item()\n",
    "        pred = outputs.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "        correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    epoch_loss = running_loss / it\n",
    "    print('Accuracy: %.4f, Loss: %.4f' % (accuracy, epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main epoch loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.lr=5e-4\n",
    "# args.epoch_per_round=3\n",
    "# args.epochs = 300\n",
    "# fed_rootmodel = create_model(network=args.network, send_to_device=False) # same random initialization\n",
    "# fed_models = { id: create_model(network=args.network, rootmodel=fed_rootmodel) for id in worker_ids }\n",
    "# fed_optimizers = { id: create_optimizer(fed_models[id], algorithm=args.optim) for id in worker_ids }\n",
    "# fed_lr_schedulers = { id: create_lr_scheduler(fed_optimizers[id], args.lr_scheduler) for id in worker_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0 on worker dave with lr=1.00e-02, wall clock 13:35:59.097268\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-51d06ef2e4a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_pysyft\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mround_complete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_federated_pysyft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfed_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfed_optimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mround_complete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_federated_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfed_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfed_optimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-45434b258ade>\u001b[0m in \u001b[0;36mtrain_federated_pysyft\u001b[1;34m(epoch, models, optimizers, global_model)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# forward/backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m#loss = criterion(outputs.logits, targets) # for googlenet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\fl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\python\\shufflenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\python\\shufflenetv2.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\fl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\fl\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\fl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\python\\shufflenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbranch1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchannel_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\python\\shufflenetv2.py\u001b[0m in \u001b[0;36mchannel_shuffle\u001b[1;34m(x, groups)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mchannel_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# type: (torch.Tensor, int) -> torch.Tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mchannels_per_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_channels\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for epoch in range(START_EPOCH, args.epochs):\n",
    "#for epoch in range(72, 120):\n",
    "    # Workaround for pysyft error: https://github.com/OpenMined/PySyft/issues/2518\n",
    "    if not args.federate:\n",
    "        train(epoch, model, optimizer)\n",
    "        epoch_loss = valid(epoch, model)\n",
    "        if args.lr_scheduler == 'plateau':\n",
    "            lr_scheduler.step(metrics=epoch_loss)\n",
    "        else:\n",
    "            lr_scheduler.step()\n",
    "    else:\n",
    "        if args.use_pysyft:\n",
    "            round_complete = train_federated_pysyft(epoch, fed_models, fed_optimizers, global_model=global_model)\n",
    "        else:\n",
    "            round_complete = train_federated_sim(epoch, fed_models, fed_optimizers, global_model=global_model)\n",
    "            \n",
    "        if round_complete:\n",
    "            epoch_loss = valid(epoch, global_model if args.global_model else fed_models[worker_ids[0]])\n",
    "            for worker_id, sched in fed_lr_schedulers.items():\n",
    "                if args.lr_scheduler == 'plateau':\n",
    "                    sched.step(metrics=epoch_loss)\n",
    "                else:\n",
    "                    sched.step()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    time_str = '[EPOCH] Total time elapsed: {:.0f}h {:.0f}m {:.0f}s '.format(time_elapsed // 3600, time_elapsed % 3600 // 60, time_elapsed % 60)\n",
    "    print(\"%s, best accuracy: %.02f%%, best loss %f\" % (time_str, 100*BEST_ACC, LEAST_LOSS))\n",
    "    \n",
    "    #time.sleep(60)\n",
    "    #if epoch % 30 == 29:\n",
    "    #    time.sleep(120) # Pause to let computer cool...\n",
    "    \n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model = torch.load('1597955978929-mobilenetv1_quantize_sgd_lambda_bs16_lr1.0e-02_wd1.0e-07-best-acc-fed(w10e3cw2).pth')\n",
    "# test(eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#federated_avg_v2(fed_models, worker_n)\n",
    "if args.federate:\n",
    "    test(global_model if args.global_model else fed_models[worker_ids[0]])\n",
    "else:\n",
    "    test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_name = '%s_%s_%s_bs%d_lr%.1e_wd%.1e' % \\\n",
    "#                     (args.network, args.optim, args.lr_scheduler, args.batch_size, args.lr, args.weight_decay)\n",
    "# torch.save(fed_models[worker_ids[0]], '%d-%s-%s.pth' % (START_TIMESTAMP, full_name, 'fed(w10e3cw5)-43ROUND'))\n",
    "# torch.save(model, '%d-%s-%s.pth' % (START_TIMESTAMP, full_name, 'centralized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if db:\n",
    "#     db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(fed_models['bob'].parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(fed_models['alice'].parameters()).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from fedavg import *\n",
    "# pickle.dump(fed_models, open('fedmodels.p', 'wb'))\n",
    "# federated_avg(fed_models, worker_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fed_models[worker_ids[1]].state_dict())\n",
    "# print(dict(fed_models[worker_ids[1]].named_parameters()).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fed_models2 = pickle.load(open('fedmodels.p', 'rb'))\n",
    "# federated_avg_v2(fed_models2, worker_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fed_models2[worker_ids[1]].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed_lr_schedulers = { id: create_lr_scheduler(fed_optimizers[id], args.lr_scheduler) for id in worker_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in worker_ids:\n",
    "#     fed_lr_schedulers[w].step()\n",
    "# get_lr(fed_optimizers[worker_ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('badmodel.dump', 'w') as f:\n",
    "#    torch.set_printoptions(profile=\"full\")\n",
    "#    d = dict(fed_models[worker_ids[9]].named_parameters())\n",
    "#    print(d, file=f)\n",
    "#    torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = torch.load('1597715469718-mobilenetv2_sgd_lambda_bs32_lr1.0e-02_wd1.0e-07-fed(w10e3cw5)-sgd.pth')\n",
    "# fed_models = { id: create_model(network=args.network, rootmodel=m) for id in worker_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model = torch.load('1597949089071-mobilenetv1_quantize_sgd_lambda_bs16_lr1.0e-02_wd1.0e-07-least-loss-fedv1(w10e3cw5).pth')\n",
    "# valid(1,eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in worker_ids:\n",
    "#     print(valid(1,fed_models[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed_optimizers[worker_ids[0]].state_dict()['state'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed_optimizers[worker_ids[0]].state_dict()['state'][2120198521064]['exp_avg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed_optimizers[worker_ids[1]].state_dict()['state'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fed_optimizers[worker_ids[1]].state_dict()['state'][2120229084600]['exp_avg'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flenv",
   "language": "python",
   "name": "flenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
